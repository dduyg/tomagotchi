{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13461546,"sourceType":"datasetVersion","datasetId":8544780}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Goal: Cluster AI agent projects by their short descriptions using dense vector embeddings.\nProduces: clean dataset, embeddings, UMAP visualizations, KMeans and HDBSCAN clusters, qualitative cluster summary, and a saved CSV with cluster labels.\n\nNotes:\n- This notebook is written in competition-style with explanations, modular code, and reproducibility in mind.\n- Replace/change paths or model names if needed. If running in an environment without `sentence-transformers`, the notebook includes an install cell.\n\n\n# 1) Introduction & Problem Summary\n\n# - Problem\n# We have a small curated CSV of AI agent projects (title, industry, description, github link). The task is unsupervised: group similar projects together so that engineers/researchers can discover clusters of agent types (e.g., information-retrieval agents, autonomous copilots, data-collection agents, RL agents, etc.).\n\n# - Motivation\n# Clusters help create taxonomies, drive RAG pipelines (select representative repos), and identify gaps or duplicates in the dataset.\n\n# - Expected challenges\n# * Short descriptions (noisy / terse) -> embeddings must be robust.\n# * Small dataset (N=71) -> prefer compact embedding models and robust clustering (HDBSCAN works well with small n).\n# * Link stability / license heterogeneity not addressed here (this notebook focuses on descriptions only).\n","metadata":{}},{"cell_type":"markdown","source":"# 2) Setup\n","metadata":{}},{"cell_type":"code","source":"# Imports, seed, and optional installs\nimport os\nimport gc\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Text and embeddings\ntry:\n    from sentence_transformers import SentenceTransformer\nexcept Exception:\n    SentenceTransformer = None\n\n# Dimensionality reduction & clustering\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\n# Optional clustering\ntry:\n    import hdbscan\nexcept Exception:\n    hdbscan = None\n\n# UMAP for visualization\ntry:\n    import umap\nexcept Exception:\n    umap = None\n\n# Reproducibility & warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\nRANDOM_SEED = 42\nnp.random.seed(RANDOM_SEED)\nrandom.seed(RANDOM_SEED)\n\n# Display settings\npd.set_option('display.max_columns', 200)\n\n# Optional: install missing libraries (uncomment if needed)\n# Note: in restricted offline environments this might fail. If so, use precomputed embeddings or run locally.\n\n# !pip install -q sentence-transformers umap-learn hdbscan\n\nprint('SentenceTransformer available?', SentenceTransformer is not None)\nprint('UMAP available?', umap is not None)\nprint('HDBSCAN available?', hdbscan is not None)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:02:44.896841Z","iopub.execute_input":"2025-11-16T09:02:44.897143Z","iopub.status.idle":"2025-11-16T09:04:02.916752Z","shell.execute_reply.started":"2025-11-16T09:02:44.897110Z","shell.execute_reply":"2025-11-16T09:04:02.915408Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3) Data Loading","metadata":{}},{"cell_type":"code","source":"\n\nDATA_PATH = Path('/kaggle/input/ai-agents-dataset-github-repositories-use-cases/agents_list.csv')\nassert DATA_PATH.exists(), f\"Dataset not found at {DATA_PATH}\"\n\ndf = pd.read_csv(DATA_PATH)\nprint('Shape:', df.shape)\nprint('\\nColumns:')\nprint(df.columns.tolist())\n\n# Quick head\nprint('\\nFirst rows:')\nprint(df.head(10).to_string(index=False))\n\n# Basic summary\nprint('\\nMissing values:')\nprint(df.isnull().sum())\n\n# Ensure descriptive column names exist; normalize\nexpected_cols = ['Use Case', 'Industry', 'Description', 'Code Github']\nfor c in expected_cols:\n    if c not in df.columns:\n        # try lowercase variants\n        for col in df.columns:\n            if col.strip().lower() == c.lower():\n                df = df.rename(columns={col: c})\n                break\n\n# If Description is missing or empty, fill with Title/Use Case\nif 'Description' not in df.columns:\n    raise ValueError('Description column missing!')\n\n# Create a text column to embed\ndf['text'] = df['Description'].fillna('')\n# fallback to Use Case or Title\nif df['text'].str.strip().eq('').any():\n    df['text'] = df['text'].mask(df['text'].str.strip()=='', df['Use Case'].fillna(''))\n\nprint('\\nSample texts:')\nprint(df['text'].head(8).to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:06:19.112291Z","iopub.execute_input":"2025-11-16T09:06:19.114003Z","iopub.status.idle":"2025-11-16T09:06:19.223554Z","shell.execute_reply.started":"2025-11-16T09:06:19.113947Z","shell.execute_reply":"2025-11-16T09:06:19.221103Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4) EDA (text)","metadata":{}},{"cell_type":"code","source":"\n\n# Length statistics\ndf['text_len'] = df['text'].str.len()\nprint('\\nText length stats:')\nprint(df['text_len'].describe())\n\nplt.figure(figsize=(8,4))\nsns.histplot(df['text_len'], bins=20)\nplt.title('Distribution of description lengths')\nplt.xlabel('Characters')\nplt.show()\n\n# Show most common industries\nif 'Industry' in df.columns:\n    print('\\nIndustry counts:')\n    print(df['Industry'].value_counts().head(20))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:06:25.285260Z","iopub.execute_input":"2025-11-16T09:06:25.285590Z","iopub.status.idle":"2025-11-16T09:06:25.692110Z","shell.execute_reply.started":"2025-11-16T09:06:25.285568Z","shell.execute_reply":"2025-11-16T09:06:25.690535Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5) Feature Engineering: Text cleaning & Embeddings","metadata":{}},{"cell_type":"code","source":"\n\n# Minimal, safe text cleaning function\nimport re\n\ndef clean_text(s):\n    if pd.isna(s):\n        return ''\n    s = str(s)\n    # Remove URLs (github links sometimes appear in text)\n    s = re.sub(r'http\\S+|www\\S+', ' ', s)\n    # Remove repeated whitespace\n    s = re.sub(r'\\s+', ' ', s)\n    s = s.strip()\n    return s\n\n# Apply\ndf['text_clean'] = df['text'].apply(clean_text)\nprint('\\nSample cleaned texts:')\nprint(df['text_clean'].head(8).to_string(index=False))\n\n# Embedding function using sentence-transformers\nEMBED_MODEL_NAME = 'all-MiniLM-L6-v2'  # compact, good for small datasets\n\nif SentenceTransformer is None:\n    raise RuntimeError('sentence-transformers is not installed in the environment. Uncomment the install cell and run again or run locally with internet access.')\n\nmodel = SentenceTransformer(EMBED_MODEL_NAME)\n\n# Compute embeddings\ntexts = df['text_clean'].tolist()\nprint('\\nComputing embeddings for', len(texts), 'items...')\nembeddings = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\nprint('Embeddings shape:', embeddings.shape)\n\n# Save embeddings to dataframe\nemb_df = pd.DataFrame(embeddings)\nemb_df.columns = [f'emb_{i}' for i in range(emb_df.shape[1])]\ndf = pd.concat([df.reset_index(drop=True), emb_df.reset_index(drop=True)], axis=1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:06:30.529008Z","iopub.execute_input":"2025-11-16T09:06:30.529512Z","iopub.status.idle":"2025-11-16T09:06:41.018761Z","shell.execute_reply.started":"2025-11-16T09:06:30.529447Z","shell.execute_reply":"2025-11-16T09:06:41.017795Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6) Dimensionality Reduction for visualization & clustering preprocessing","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade umap-learn\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:07:48.084132Z","iopub.execute_input":"2025-11-16T09:07:48.084579Z","iopub.status.idle":"2025-11-16T09:07:57.898562Z","shell.execute_reply.started":"2025-11-16T09:07:48.084548Z","shell.execute_reply":"2025-11-16T09:07:57.897240Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# PCA to reduce to 50 dims (for clustering stability / HDBSCAN)\nN_PCA = min(50, embeddings.shape[1])\nprint('Running PCA ->', N_PCA, 'components')\n\n\npca = PCA(n_components=N_PCA, random_state=RANDOM_SEED)\nemb_pca = pca.fit_transform(embeddings)\nprint('PCA explained variance ratio (sum):', pca.explained_variance_ratio_.sum())\n\n\n# UMAP for 2D visualization\nif umap is None:\n    raise RuntimeError('umap-learn not installed. Uncomment install cell or run locally.')\n\n\n# 2D PCA Dimensionality Reduction (UMAP disabled due to version conflict)\npca_2d = PCA(n_components=2, random_state=RANDOM_SEED).fit_transform(emb_pca)\nemb_2d = pca_2d\nprint('2D PCA shape:', emb_2d.shape)\nprint('UMAP shape:', emb_2d.shape)\n\n\n# Append to df\ndf['umap_0'] = emb_2d[:,0]\ndf['umap_1'] = emb_2d[:,1]\n\n\nplt.figure(figsize=(8,6))\nplt.scatter(df['umap_0'], df['umap_1'], s=60)\nfor i, txt in enumerate(df['Use Case'].fillna('').tolist()):\n    if i < 30: # label only first 30 points to reduce clutter\n        plt.text(df.loc[i,'umap_0']+0.01, df.loc[i,'umap_1']+0.01, str(i)+': '+txt, fontsize=8)\nplt.title('UMAP projection of agent descriptions')\nplt.xlabel('UMAP 0')\nplt.ylabel('UMAP 1')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:15:36.875042Z","iopub.execute_input":"2025-11-16T09:15:36.875855Z","iopub.status.idle":"2025-11-16T09:15:37.241721Z","shell.execute_reply.started":"2025-11-16T09:15:36.875825Z","shell.execute_reply":"2025-11-16T09:15:37.240606Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7) Clustering: KMeans (baseline) + HDBSCAN (density-based)\n","metadata":{}},{"cell_type":"code","source":"\n\n\n# Helper to evaluate KMeans for range of k\nfrom sklearn.metrics import pairwise_distances\n\ndef kmeans_explore(X, k_min=2, k_max=12, random_state=RANDOM_SEED):\n    inertia = []\n    sil_scores = []\n    K = list(range(k_min, k_max+1))\n    for k in K:\n        km = KMeans(n_clusters=k, random_state=random_state, n_init=20)\n        labels = km.fit_predict(X)\n        inertia.append(km.inertia_)\n        if len(set(labels))>1:\n            sil_scores.append(silhouette_score(X, labels))\n        else:\n            sil_scores.append(np.nan)\n    return K, inertia, sil_scores\n\nK, inertia, sil_scores = kmeans_explore(emb_pca, k_min=2, k_max=12)\n\nplt.figure(figsize=(12,4))\nplt.subplot(1,2,1)\nplt.plot(K, inertia, '-o')\nplt.xlabel('k')\nplt.ylabel('Inertia')\nplt.title('KMeans elbow')\n\nplt.subplot(1,2,2)\nplt.plot(K, sil_scores, '-o')\nplt.xlabel('k')\nplt.ylabel('Silhouette score')\nplt.title('KMeans silhouette')\nplt.show()\n\n# Choose k by inspecting plots; default to k=6 (example) but we'll pick the best silhouette\nbest_k_idx = np.nanargmax(sil_scores)\nbest_k = K[best_k_idx]\nprint('Best K by silhouette:', best_k)\n\n# Fit final KMeans\nkmeans = KMeans(n_clusters=best_k, random_state=RANDOM_SEED, n_init=50)\nklabels = kmeans.fit_predict(emb_pca)\ndf['kmeans_cluster'] = klabels\n\nplt.figure(figsize=(8,6))\npalette = sns.color_palette('tab10', best_k)\nfor c in range(best_k):\n    sel = df[df['kmeans_cluster']==c]\n    plt.scatter(sel['umap_0'], sel['umap_1'], s=70, label=f'cluster {c}')\nplt.legend()\nplt.title(f'KMeans clusters (k={best_k}) on UMAP')\nplt.show()\n\n# HDBSCAN (if available) - often finds meaningful clusters and noise points\nif hdbscan is None:\n    raise RuntimeError('hdbscan not installed. Uncomment install cell or run locally to enable HDBSCAN.')\n\nclusterer = hdbscan.HDBSCAN(min_cluster_size=3, min_samples=1, metric='euclidean')\nhdb_labels = clusterer.fit_predict(emb_pca)\n\ndf['hdbscan_cluster'] = hdb_labels\n\nprint('HDBSCAN cluster counts (including -1 noise):')\nprint(pd.Series(hdb_labels).value_counts())\n\nplt.figure(figsize=(8,6))\nsns.scatterplot(x='umap_0', y='umap_1', hue='hdbscan_cluster', data=df, palette='tab10', s=80)\nplt.title('HDBSCAN clusters on UMAP')\nplt.legend(bbox_to_anchor=(1.05,1), loc='upper left')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:15:48.207282Z","iopub.execute_input":"2025-11-16T09:15:48.207635Z","iopub.status.idle":"2025-11-16T09:15:49.973282Z","shell.execute_reply.started":"2025-11-16T09:15:48.207611Z","shell.execute_reply":"2025-11-16T09:15:49.972140Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 8) Cluster analysis & qualitative summaries","metadata":{}},{"cell_type":"code","source":"pip install --upgrade pydantic\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:21:56.864406Z","iopub.execute_input":"2025-11-16T09:21:56.864774Z","iopub.status.idle":"2025-11-16T09:22:01.449562Z","shell.execute_reply.started":"2025-11-16T09:21:56.864751Z","shell.execute_reply":"2025-11-16T09:22:01.448220Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n\n\n# Function to show representative items per cluster\n\n\ndef cluster_summary(df, cluster_col='hdbscan_cluster', n_top=8):\n    clusters = sorted(df[cluster_col].unique())\n    summary = {}\n    for c in clusters:\n        if c == -1:\n            label = 'noise'\n        else:\n            label = str(c)\n        sub = df[df[cluster_col]==c]\n# pick top n by text length (proxy for richer description) or random if short\n        rep = sub.sort_values('text_len', ascending=False).head(n_top)[['Use Case','Industry','text_clean','Code Github']]\n        summary[c] = rep\n    return summary\n\n\nsumm = cluster_summary(df, cluster_col='hdbscan_cluster', n_top=6)\n\n\n# Print brief summaries for non-noise clusters\nfor c, rep in summ.items():\n    print('\\n' + '='*60)\n    print('Cluster:', c, '| Size:', len(df[df['hdbscan_cluster']==c]))\n    print(rep.to_string(index=False))\n\n\n# Compute cluster-level keywords using simple token frequency (very lightweight)\nfrom collections import Counter\nimport math\n\n\n# NLTK removed â€” not required for embedding-based clustering\n\n\nfrom nltk.tokenize import word_tokenize\n\n\n# Create tokens for each description\nSTOP = set([w.strip().lower() for w in ['the','a','an','and','or','for','to','of','in','on','with','by','from','as','is','are']])\n\n\ndef top_keywords_for_cluster(df, cluster_col='hdbscan_cluster', topn=8):\n    kc = {}\n    for c in sorted(df[cluster_col].unique()):\n        texts = df.loc[df[cluster_col]==c, 'text_clean'].tolist()\n        tokens = []\n        for t in texts:\n            toks = [w.lower() for w in word_tokenize(t) if w.isalpha()]\n            tokens.extend([w for w in toks if w not in STOP and len(w)>2])\n        counts = Counter(tokens)\n        most = counts.most_common(topn)\n        kc[c] = most\n    return kc\n\n\nkw = top_keywords_for_cluster(df, 'hdbscan_cluster', topn=10)\nprint('\\nTop keywords per HDBSCAN cluster:')\nfor c,v in kw.items():\n    print('Cluster', c, ':', ', '.join([f'{w}({n})' for w,n in v]))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:22:07.712105Z","iopub.execute_input":"2025-11-16T09:22:07.712549Z","iopub.status.idle":"2025-11-16T09:22:07.863089Z","shell.execute_reply.started":"2025-11-16T09:22:07.712505Z","shell.execute_reply":"2025-11-16T09:22:07.861420Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 9) Save results","metadata":{}},{"cell_type":"code","source":"\n\n\nOUT_CSV = Path('agents_clusters.csv')\ncols_to_save = ['Use Case','Industry','Description','Code Github','text_clean','kmeans_cluster','hdbscan_cluster','umap_0','umap_1']\ndf[cols_to_save].to_csv(OUT_CSV, index=False)\nprint('\\nSaved cluster results to', OUT_CSV)\n\n# Also save embeddings (optional)\nEMB_OUT = Path('agents_embeddings.npy')\nnp.save(EMB_OUT, embeddings)\nprint('Saved embeddings to', EMB_OUT)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:25:45.227298Z","iopub.execute_input":"2025-11-16T09:25:45.227687Z","iopub.status.idle":"2025-11-16T09:25:45.246209Z","shell.execute_reply.started":"2025-11-16T09:25:45.227662Z","shell.execute_reply":"2025-11-16T09:25:45.245037Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 10) Full Explanation and Next Steps (competition-style notes)\n","metadata":{}},{"cell_type":"code","source":"\n# Why these choices:\n# - SentenceTransformer 'all-MiniLM-L6-v2' is compact and effective for semantic similarity with small datasets.\n# - PCA before clustering stabilizes distances and reduces noise; 50 dims is a common sweet spot.\n# - UMAP gives a human-friendly 2D projection for visual inspection. Parameters can be tuned for tighter clusters.\n# - KMeans provides a simple baseline and well-understood cluster centroids; silhouette score helps pick k.\n# - HDBSCAN is density-based, robust to clusters of different shapes/sizes and returns a -1 label for noise.\n\n# Limitations & advice:\n# 1. This notebook uses only the short description. Better results come from combining README, topics, and code comments.\n# 2. For production use in RAG systems, fetch the repo README and source, chunk texts, and build an index (FAISS/Chroma).\n# 3. Try alternative embedding models: larger SBERT variants, OpenAI embeddings (text-embedding-3-large), or fine-tuned domain models.\n# 4. Tune UMAP/HDBSCAN hyperparameters. Use GridSearch-style sweeps for min_cluster_size / min_samples.\n# 5. Use cross-checks with stars/README length to validate cluster coherence.\n\n# Quick pointers to improve:\n# - Replace `top_keywords_for_cluster` with TF-IDF + top terms from cluster centroid.\n# - Add qualitative human-in-the-loop labeling for a few clusters to bootstrap a classifier.\n# - If you want an interactive scatter with hover tooltips, export `umap_0, umap_1` and `Use Case` to a small HTML via plotly.\n\nprint('\\nNotebook complete. Review clusters, inspect the saved CSV, and iterate on model / parameters.')\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-16T09:25:54.916321Z","iopub.execute_input":"2025-11-16T09:25:54.916674Z","iopub.status.idle":"2025-11-16T09:25:54.923256Z","shell.execute_reply.started":"2025-11-16T09:25:54.916650Z","shell.execute_reply":"2025-11-16T09:25:54.922242Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}